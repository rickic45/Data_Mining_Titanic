{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rickic45/Data_Mining_Titanic/blob/main/data_mining_titanic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Understanding & Preparation\n"
      ],
      "metadata": {
        "id": "uBDgkZnyySSf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jsUsuu2AlsUz"
      },
      "outputs": [],
      "source": [
        "# Import all necessary libraries for our analysis. We will use:\n",
        "\n",
        "import numpy as np              # For mathematical operations and array manipulation\n",
        "import pandas as pd             # For handling and analyzing tabular data\n",
        "import matplotlib.pyplot as plt # For creating plots\n",
        "import seaborn as sns           # For advanced statistical visualizations\n",
        "\n",
        "# Specific machine learning libraries from scikit-learn\n",
        "from sklearn import datasets                                    # To load datasets\n",
        "from sklearn.model_selection import train_test_split            # To split data into training and test sets\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score # For hyperparameter tuning and validation\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler  # For data normalization\n",
        "from sklearn.decomposition import PCA                           # For dimensionality reduction\n",
        "from sklearn.pipeline import Pipeline                           # To create preprocessing+model pipelines\n",
        "\n",
        "# Evaluation metrics\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Classification models\n",
        "from sklearn.linear_model import LogisticRegression    # Logistic Regression\n",
        "from sklearn.neighbors import KNeighborsClassifier     # K-Nearest Neighbors\n",
        "from sklearn.svm import SVC                            # Support Vector Machine\n",
        "from sklearn.tree import DecisionTreeClassifier        # Decision Tree\n",
        "from sklearn.ensemble import (                         # Ensemble methods\n",
        "    RandomForestClassifier,\n",
        "    GradientBoostingClassifier\n",
        ")\n",
        "from sklearn.neural_network import MLPClassifier      # Multi-layer Perceptron (neural networks)\n",
        "\n",
        "# To save models\n",
        "import joblib\n",
        "\n",
        "# To ignore warnings (optional)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Titanic\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "df = pd.read_csv(\"titanic.csv\")\n",
        "\n",
        "# Create new features\n",
        "df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
        "df['IsAlone'] = (df['FamilySize'] == 1).astype(int)\n",
        "df['HasCabin'] = df['Cabin'].notnull().astype(int)\n",
        "\n",
        "# Extract Title from Name\n",
        "df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
        "df['Title'] = df['Title'].replace(['Mlle', 'Ms'], 'Miss')\n",
        "df['Title'] = df['Title'].replace(['Mme', 'Lady', 'Countess', 'Dona'], 'Mrs')\n",
        "df['Title'] = df['Title'].replace(['Capt', 'Col', 'Major', 'Dr', 'Rev', 'Jonkheer', 'Don', 'Sir'], 'Rare')\n",
        "\n",
        "# Drop unused columns\n",
        "df = df.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "SrVPVwgowznb",
        "outputId": "f5e5becd-24fc-4898-ba9a-fad327b01d86"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-33314037-1f24-47a9-9e48-0f060df06a12\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-33314037-1f24-47a9-9e48-0f060df06a12\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-26442128aa5a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load Titanic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"titanic.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    165\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    166\u001b[0m           \u001b[0minput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing values\n",
        "print(\"Missing values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Duplicates\n",
        "print(\"Duplicates:\", df.duplicated().sum())\n",
        "\n",
        "# Columns with same value\n",
        "constant_cols = [col for col in df.columns if df[col].nunique() == 1]\n",
        "print(\"Constant columns:\", constant_cols)\n"
      ],
      "metadata": {
        "id": "t4N6kr10xQU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Numeric distributions\n",
        "df.hist(bins=20, figsize=(15, 10))\n",
        "plt.suptitle(\"Histograms of Numeric Variables\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gbWb7v1kxVMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# List of numerical columns to plot\n",
        "numerical_columns = ['Age', 'Fare', 'SibSp', 'Parch']\n",
        "\n",
        "# Set up the figure size and layout for 4 subplots (vertically arranged)\n",
        "fig, axes = plt.subplots(4, 1, figsize=(8, 16))\n",
        "\n",
        "# Create a boxplot for each of the numerical columns\n",
        "sns.boxplot(data=df, y='Age', palette='Set2', ax=axes[0])\n",
        "axes[0].set_title('Boxplot for Age')\n",
        "axes[0].set_xlabel('Values')\n",
        "axes[0].set_ylabel('Age')\n",
        "\n",
        "sns.boxplot(data=df, y='Fare', palette='Set2', ax=axes[1])\n",
        "axes[1].set_title('Boxplot for Fare')\n",
        "axes[1].set_xlabel('Values')\n",
        "axes[1].set_ylabel('Fare')\n",
        "\n",
        "sns.boxplot(data=df, y='SibSp', palette='Set2', ax=axes[2])\n",
        "axes[2].set_title('Boxplot for SibSp')\n",
        "axes[2].set_xlabel('Values')\n",
        "axes[2].set_ylabel('SibSp')\n",
        "\n",
        "sns.boxplot(data=df, y='Parch', palette='Set2', ax=axes[3])\n",
        "axes[3].set_title('Boxplot for Parch')\n",
        "axes[3].set_xlabel('Values')\n",
        "axes[3].set_ylabel('Parch')\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Z_xrfE_-xded"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Set the correlation threshold (e.g., 0.9)\n",
        "threshold = 0.9\n",
        "\n",
        "# Create a correlation matrix\n",
        "corr = df.corr(numeric_only=True)\n",
        "\n",
        "# Find the columns with high correlation\n",
        "high_corr_columns = [column for column in corr.columns if any(abs(corr[column]) > threshold) and column not in corr.columns]\n",
        "\n",
        "# Check how many columns were dropped\n",
        "print(f\"Columns with correlation higher than {threshold}: {high_corr_columns}\")\n",
        "\n",
        "# If no columns were dropped, continue with the original df\n",
        "if high_corr_columns:\n",
        "    df_reduced = df.drop(columns=high_corr_columns)\n",
        "else:\n",
        "    print(\"No columns were removed. The dataset may already be sparse.\")\n",
        "    df_reduced = df\n",
        "\n",
        "# Plot the correlation matrix for the reduced dataframe (only if the reduced dataframe is not empty)\n",
        "if not df_reduced.empty:\n",
        "    # Create the correlation matrix for the reduced dataframe\n",
        "    corr_reduced = df_reduced.corr(numeric_only=True)\n",
        "\n",
        "    # Set figure size for better readability\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Use a different color map and adjust the font size of annotations\n",
        "    sns.heatmap(corr_reduced, annot=True, fmt=\".2f\", cmap='viridis', annot_kws={'size': 10},\n",
        "                cbar_kws={'shrink': 0.8}, linewidths=0.5, square=True)\n",
        "\n",
        "    # Title and display\n",
        "    plt.title(f\"Reduced Correlation Matrix (Threshold > {threshold})\", fontsize=16)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No data to plot. The dataframe is empty after applying the threshold.\")\n"
      ],
      "metadata": {
        "id": "iGwPZCH5xivw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()\n"
      ],
      "metadata": {
        "id": "DeRU5jADx9iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_encoded = pd.get_dummies(df, drop_first=True)\n"
      ],
      "metadata": {
        "id": "RRXTP7e2yBkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['log_Fare'] = np.log1p(df['Fare'])  # example on Fare\n"
      ],
      "metadata": {
        "id": "PMykSvlZyEen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"titanic_cleaned.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "MVf4VtikyHdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(df[['Age', 'Fare', 'Pclass', 'SibSp', 'Parch']], diag_kind='kde')\n"
      ],
      "metadata": {
        "id": "t0G7zBoI1fY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_corr = df.copy()\n",
        "df_corr['Survived'] = pd.read_csv(\"titanic.csv\")['Survived']  # if missing\n",
        "corr_target = df_corr.corr(numeric_only=True)['Survived'].sort_values(ascending=False)\n",
        "print(corr_target)\n"
      ],
      "metadata": {
        "id": "X2TZ7eLX1k1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clustering"
      ],
      "metadata": {
        "id": "gS7OZr7UyaJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the cleaned dataset\n",
        "df = pd.read_csv(\"titanic.csv\")\n",
        "\n",
        "# Drop rows with nulls in relevant columns\n",
        "df_clust = df[['Age', 'Fare', 'Pclass', 'SibSp', 'Parch']].dropna()\n",
        "\n",
        "# Normalize features (important for distance-based clustering)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(df_clust)\n"
      ],
      "metadata": {
        "id": "b5xJYzqryf3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "inertia = []\n",
        "k_range = range(1, 11)\n",
        "\n",
        "for k in k_range:\n",
        "    km = KMeans(n_clusters=k, random_state=42)\n",
        "    km.fit(X_scaled)\n",
        "    inertia.append(km.inertia_)\n",
        "\n",
        "# Plot elbow curve\n",
        "plt.plot(k_range, inertia, marker='o')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Inertia')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6x5JVnOGykfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "df_clust['kmeans_cluster'] = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# View cluster means\n",
        "print(df_clust.groupby('kmeans_cluster').mean())\n"
      ],
      "metadata": {
        "id": "-z9oBg5BypDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "\n",
        "# Use NearestNeighbors to guess epsilon\n",
        "neigh = NearestNeighbors(n_neighbors=5)\n",
        "nbrs = neigh.fit(X_scaled)\n",
        "distances, _ = nbrs.kneighbors(X_scaled)\n",
        "\n",
        "# Sort and plot distances\n",
        "distances = np.sort(distances[:, 4])\n",
        "plt.plot(distances)\n",
        "plt.title(\"K-distance Graph for DBSCAN\")\n",
        "plt.xlabel(\"Points\")\n",
        "plt.ylabel(\"Distance to 5th nearest neighbor\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QYZ0SuEbysTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = DBSCAN(eps=0.5, min_samples=5)\n",
        "df_clust['dbscan_cluster'] = db.fit_predict(X_scaled)\n",
        "\n",
        "# Count cluster labels\n",
        "print(df_clust['dbscan_cluster'].value_counts())\n"
      ],
      "metadata": {
        "id": "O2uiARk1yvKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# K-Means plot\n",
        "plt.figure()\n",
        "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=df_clust['kmeans_cluster'], palette='Set2')\n",
        "plt.title(\"K-Means Clustering (2D PCA)\")\n",
        "plt.show()\n",
        "\n",
        "# DBSCAN plot\n",
        "plt.figure()\n",
        "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=df_clust['dbscan_cluster'], palette='Set1')\n",
        "plt.title(\"DBSCAN Clustering (2D PCA)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uFmzDmW_yyZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x='kmeans_cluster', data=df_clust)\n",
        "plt.title(\"K-Means Cluster Distribution\")\n"
      ],
      "metadata": {
        "id": "6G-7RcMe115c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in ['Age', 'Fare']:\n",
        "    sns.violinplot(x='kmeans_cluster', y=col, data=df_clust)\n",
        "    plt.title(f\"{col} by Cluster\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "ziuUZYV614QX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification"
      ],
      "metadata": {
        "id": "-a8nEP1xzRKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"titanic.csv\")\n",
        "\n",
        "# Drop rows with missing values in key columns\n",
        "df = df[['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']].dropna()\n",
        "\n",
        "# One-hot encode categorical features\n",
        "df = pd.get_dummies(df, columns=['Sex', 'Embarked'], drop_first=True)\n",
        "\n",
        "# Split X and y\n",
        "X = df.drop('Survived', axis=1)\n",
        "y = df['Survived']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "YxTG46_LzR3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "y_pred_tree = tree.predict(X_test)\n",
        "print(\"Decision Tree:\\n\", classification_report(y_test, y_pred_tree))\n"
      ],
      "metadata": {
        "id": "kubMYnnmzXMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# K-Nearest Neighbors (KNN)\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "y_pred_knn = knn.predict(X_test)\n",
        "print(\"KNN:\\n\", classification_report(y_test, y_pred_knn))\n"
      ],
      "metadata": {
        "id": "NcL424iWzcPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive Bayes\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "y_pred_nb = nb.predict(X_test)\n",
        "print(\"Naive Bayes:\\n\", classification_report(y_test, y_pred_nb))\n"
      ],
      "metadata": {
        "id": "VPJoECHOzpY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n",
        "\n",
        "\n",
        "# Example for Decision Tree\n",
        "cm = confusion_matrix(y_test, y_pred_tree)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm).plot()\n",
        "plt.title(\"Decision Tree Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# ROC Curve\n",
        "y_proba_tree = tree.predict_proba(X_test)[:, 1]\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba_tree)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.plot(fpr, tpr, label=f'Decision Tree (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], '--')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Amw7_rfEzxb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import plot_tree\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plot_tree(tree, feature_names=X.columns, class_names=[\"No\", \"Yes\"], filled=True)\n",
        "plt.title(\"Decision Tree Structure\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "E8g3-hTVz-Yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pattern Mining"
      ],
      "metadata": {
        "id": "1bhEPXZu0TGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"titanic.csv\")\n",
        "\n",
        "# Keep only relevant + categorical features\n",
        "df = df[['Survived', 'Pclass', 'Sex', 'Embarked']].dropna()\n",
        "\n",
        "# Convert to string type for transaction encoding\n",
        "df = df.astype(str)\n",
        "\n",
        "# One-hot encode\n",
        "df_encoded = pd.get_dummies(df)\n"
      ],
      "metadata": {
        "id": "WI-8AiJK0T3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mlxtend.frequent_patterns import apriori\n",
        "\n",
        "# Find frequent itemsets with min support\n",
        "frequent_itemsets = apriori(df_encoded, min_support=0.1, use_colnames=True)\n",
        "\n",
        "# Sort by length of itemsets\n",
        "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
        "print(frequent_itemsets.sort_values(by=['support', 'length'], ascending=False))\n"
      ],
      "metadata": {
        "id": "ELD67bTX0Wix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mlxtend.frequent_patterns import association_rules\n",
        "\n",
        "rules = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.6)\n",
        "\n",
        "# Sort by confidence and lift\n",
        "rules_sorted = rules.sort_values(by=['lift', 'confidence'], ascending=False)\n",
        "print(rules_sorted[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n"
      ],
      "metadata": {
        "id": "s5vsjwP40azC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Histogram of confidence\n",
        "plt.figure()\n",
        "sns.histplot(rules['confidence'], bins=10, kde=False)\n",
        "plt.title(\"Confidence Distribution\")\n",
        "plt.xlabel(\"Confidence\")\n",
        "plt.show()\n",
        "\n",
        "# Histogram of lift\n",
        "plt.figure()\n",
        "sns.histplot(rules['lift'], bins=10, kde=False)\n",
        "plt.title(\"Lift Distribution\")\n",
        "plt.xlabel(\"Lift\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xu0_IMFl0eeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = pd.read_csv(\"titanic.csv\")\n",
        "\n",
        "# Keep and drop missing\n",
        "df = df[['Fare', 'Pclass', 'Age', 'SibSp', 'Parch', 'Sex', 'Embarked']].dropna()\n",
        "\n",
        "# One-hot encode\n",
        "df = pd.get_dummies(df, columns=['Sex', 'Embarked'], drop_first=True)\n",
        "\n",
        "# Split\n",
        "X = df.drop('Fare', axis=1)\n",
        "y = df['Fare']\n"
      ],
      "metadata": {
        "id": "c6NeIbnh02QC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(data=rules, x='confidence', y='lift', size='support', hue='support', sizes=(20,200))\n",
        "plt.title(\"Rules: Confidence vs Lift\")\n"
      ],
      "metadata": {
        "id": "dhoFKEGK2HAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_rules = rules.sort_values('lift', ascending=False).head(10)\n",
        "for _, row in top_rules.iterrows():\n",
        "    print(f\"{set(row['antecedents'])} => {set(row['consequents'])} | Lift={row['lift']:.2f}\")\n"
      ],
      "metadata": {
        "id": "lNJFoPZG2J7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n",
        "\n",
        "# Model\n",
        "reg = LinearRegression()\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"R²:\", r2_score(y_test, y_pred))\n",
        "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
        "print(\"Coefficients:\", list(zip(X.columns, reg.coef_)))\n"
      ],
      "metadata": {
        "id": "GSNExl_A08SB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge, Lasso\n",
        "\n",
        "# Ridge\n",
        "ridge = Ridge(alpha=1.0)\n",
        "ridge.fit(X_train, y_train)\n",
        "print(\"Ridge R²:\", ridge.score(X_test, y_test))\n",
        "\n",
        "# Lasso\n",
        "lasso = Lasso(alpha=0.1)\n",
        "lasso.fit(X_train, y_train)\n",
        "print(\"Lasso R²:\", lasso.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "7_Mqltsb1Ai7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "tree = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "y_tree_pred = tree.predict(X_test)\n",
        "\n",
        "print(\"Tree R²:\", r2_score(y_test, y_tree_pred))\n"
      ],
      "metadata": {
        "id": "b9a4Yh8W1EjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.scatter(y_test, y_pred, alpha=0.5)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
        "plt.xlabel(\"Actual Fare\")\n",
        "plt.ylabel(\"Predicted Fare\")\n",
        "plt.title(\"Prediction vs Actual\")\n"
      ],
      "metadata": {
        "id": "OQJeIavb2Njz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "residuals = y_test - y_pred\n",
        "sns.histplot(residuals, bins=20, kde=True)\n",
        "plt.title(\"Residuals Histogram\")\n"
      ],
      "metadata": {
        "id": "9jvt-C2J2Qgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# ... (Your previous code for model training and prediction) ...\n",
        "\n",
        "# Store evaluation metrics for each model\n",
        "r2_linear = r2_score(y_test, y_pred)  # Assuming y_pred is from Linear Regression\n",
        "mse_linear = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "r2_ridge = ridge.score(X_test, y_test)  # Assuming ridge is your Ridge model\n",
        "mse_ridge = mean_squared_error(y_test, ridge.predict(X_test))\n",
        "\n",
        "r2_lasso = lasso.score(X_test, y_test)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "A4wswvgi5YIq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}